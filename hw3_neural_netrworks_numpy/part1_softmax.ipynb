{"cells":[{"cell_type":"markdown","metadata":{"id":"HR7Ypc5mxbRF"},"source":["## Homework 2. Part 1. Softmax classifier (logistic regression) from scratch.\n","\n","### Set COLAB to True if you work in COLAB, else, set it to False"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":139,"status":"ok","timestamp":1676560898396,"user":{"displayName":"Chris Winsor","userId":"06032669675435120814"},"user_tz":300},"id":"bYg4CLFQxbRI"},"outputs":[],"source":["# YOUR CODE HERE\n","COLAB = True"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4087,"status":"ok","timestamp":1676560906585,"user":{"displayName":"Chris Winsor","userId":"06032669675435120814"},"user_tz":300},"id":"isaejnSJxbRI","outputId":"f777f15b-6766-4e4d-c817-0ff394d9ec41"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/My Drive/comp5300/assignments/hw3_neural_netrworks_numpy/cs231n/datasets\n","/content/drive/My Drive/comp5300/assignments/hw3_neural_netrworks_numpy\n"]}],"source":["if COLAB:\n","    # This mounts your Google Drive to the Colab VM.\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","\n","    # Enter the foldername in your Drive where you have saved the unzipped\n","    # assignment folder, e.g. 'cs231n/assignments/assignment1/'\n","    # FOLDERNAME = None\n","    FOLDERNAME = 'comp5300/assignments/hw3_neural_netrworks_numpy/'\n","    assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","    # Now that we've mounted your Drive, this ensures that\n","    # the Python interpreter of the Colab VM can load\n","    # python files from within it.\n","    import sys\n","    sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","\n","    # This downloads the CIFAR-10 dataset to your Drive\n","    # if it doesn't already exist.\n","    %cd drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n","    !bash get_datasets.sh\n","    %cd /content/drive/My\\ Drive/$FOLDERNAME\n","else:\n","    %cd cs231n/datasets/\n","    !bash get_datasets.sh\n","    %cd ../.."]},{"cell_type":"markdown","metadata":{"id":"AMkRQbDdxbRJ","tags":["pdf-title"]},"source":["# Softmax exercise\n","\n","*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n","\n","In this exercise you will:\n","\n","- implement a fully-vectorized **loss function** for the Softmax classifier\n","- implement the fully-vectorized expression for its **analytic gradient**\n","- **check your implementation** with numerical gradient\n","- use a validation set to **tune the learning rate and regularization** strength\n","- **optimize** the loss function with **SGD**\n","- **visualize** the final learned weights\n","\n","Credit: Stanford cs231n\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":114,"status":"ok","timestamp":1676560936750,"user":{"displayName":"Chris Winsor","userId":"06032669675435120814"},"user_tz":300},"id":"GT29rm89xbRJ","tags":["pdf-ignore"]},"outputs":[],"source":["import random\n","import numpy as np\n","from cs231n.data_utils import load_CIFAR10\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# for auto-reloading extenrnal modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["a=5\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3938,"status":"ok","timestamp":1676560961892,"user":{"displayName":"Chris Winsor","userId":"06032669675435120814"},"user_tz":300},"id":"zCeS-52AxbRK","outputId":"2eec85fa-945f-4c77-f5bf-d34bdd1f5d46","tags":["pdf-ignore"]},"outputs":[{"name":"stdout","output_type":"stream","text":["Train data shape:  (49000, 3073)\n","Train labels shape:  (49000,)\n","Validation data shape:  (1000, 3073)\n","Validation labels shape:  (1000,)\n","Test data shape:  (1000, 3073)\n","Test labels shape:  (1000,)\n","dev data shape:  (500, 3073)\n","dev labels shape:  (500,)\n"]}],"source":["def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n","    \"\"\"\n","    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n","    it for the linear classifier. These are the same steps as we used for the\n","    SVM, but condensed to a single function.  \n","    \"\"\"\n","    # Load the raw CIFAR-10 data\n","    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n","    \n","    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n","    try:\n","       del X_train, y_train\n","       del X_test, y_test\n","       print('Clear previously loaded data.')\n","    except:\n","       pass\n","\n","    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n","    \n","    # subsample the data\n","    mask = list(range(num_training, num_training + num_validation))\n","    X_val = X_train[mask]\n","    y_val = y_train[mask]\n","    mask = list(range(num_training))\n","    X_train = X_train[mask]\n","    y_train = y_train[mask]\n","    mask = list(range(num_test))\n","    X_test = X_test[mask]\n","    y_test = y_test[mask]\n","    mask = np.random.choice(num_training, num_dev, replace=False)\n","    X_dev = X_train[mask]\n","    y_dev = y_train[mask]\n","    \n","    # Preprocessing: reshape the image data into rows\n","    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n","    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n","    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n","    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n","    \n","    # Normalize the data: subtract the mean image\n","    mean_image = np.mean(X_train, axis = 0)\n","    X_train -= mean_image\n","    X_val -= mean_image\n","    X_test -= mean_image\n","    X_dev -= mean_image\n","    \n","    # add bias dimension and transform into columns\n","    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n","    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n","    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n","    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n","    \n","    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n","\n","\n","# Invoke the above function to get our data.\n","X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n","print('Train data shape: ', X_train.shape)\n","print('Train labels shape: ', y_train.shape)\n","print('Validation data shape: ', X_val.shape)\n","print('Validation labels shape: ', y_val.shape)\n","print('Test data shape: ', X_test.shape)\n","print('Test labels shape: ', y_test.shape)\n","print('dev data shape: ', X_dev.shape)\n","print('dev labels shape: ', y_dev.shape)"]},{"cell_type":"markdown","metadata":{"id":"fvRmMKpBxbRK"},"source":["## Softmax Classifier\n","\n","Your code for this section will all be written inside `cs231n/classifiers/softmax.py`.\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14482,"status":"ok","timestamp":1676561011197,"user":{"displayName":"Chris Winsor","userId":"06032669675435120814"},"user_tz":300},"id":"f1b5w8VUxbRL","outputId":"aff22605-ed19-429c-9740-870b017e2030"},"outputs":[{"name":"stdout","output_type":"stream","text":["loss: 2.399137\n","sanity check: 2.302585\n"]}],"source":["# First implement the naive softmax loss function with nested loops.\n","# Open the file cs231n/classifiers/softmax.py and implement the\n","# softmax_loss_naive function.\n","\n","from cs231n.classifiers.softmax import softmax_loss_naive\n","import time\n","\n","# Generate a random softmax weight matrix and use it to compute the loss.\n","W = np.random.randn(3073, 10) * 0.0001\n","loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n","\n","# As a rough sanity check, our loss should be something close to -log(0.1).\n","print('loss: %f' % loss)\n","print('sanity check: %f' % (-np.log(0.1)))"]},{"cell_type":"markdown","metadata":{"id":"GinWfouNxbRL","tags":["pdf-inline"]},"source":["**Inline Question 1**\n","\n","Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n","\n","**Answer:**\n","\n","The number of target classes is 10, and with randomly initialized weights \n","we would expect 10% correct predictions. Since the cross-entropy equation\n","is 1.0 * ln(probability(x))  that is  ln(0.1)  which is the sanity check.\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":538105,"status":"ok","timestamp":1676561609675,"user":{"displayName":"Chris Winsor","userId":"06032669675435120814"},"user_tz":300},"id":"5bIRxh2GxbRM","outputId":"6ca73e0a-137d-4def-e454-c7ab403fa853"},"outputs":[{"name":"stdout","output_type":"stream","text":["numerical: 1.443155 analytic: 0.000000, relative error: 1.000000e+00\n","numerical: 4.718519 analytic: 0.000000, relative error: 1.000000e+00\n","numerical: -2.301830 analytic: 0.000000, relative error: 1.000000e+00\n","numerical: 0.716855 analytic: 0.000000, relative error: 1.000000e+00\n","numerical: 0.857495 analytic: 0.000000, relative error: 1.000000e+00\n","numerical: 2.574704 analytic: 0.000000, relative error: 1.000000e+00\n","numerical: -1.154241 analytic: 0.000000, relative error: 1.000000e+00\n","numerical: -0.678544 analytic: 0.000000, relative error: 1.000000e+00\n","numerical: -3.425327 analytic: 0.000000, relative error: 1.000000e+00\n","numerical: 0.834094 analytic: 0.000000, relative error: 1.000000e+00\n","numerical: -0.815487 analytic: 0.000000, relative error: 1.000000e+00\n","numerical: -0.497041 analytic: 0.000000, relative error: 1.000000e+00\n","numerical: -0.816866 analytic: 0.000000, relative error: 1.000000e+00\n","numerical: -0.900850 analytic: 0.000000, relative error: 1.000000e+00\n","numerical: -1.004447 analytic: 0.000000, relative error: 1.000000e+00\n","numerical: -2.250649 analytic: 0.000000, relative error: 1.000000e+00\n","numerical: -0.251736 analytic: 0.000000, relative error: 1.000000e+00\n","numerical: -1.330586 analytic: 0.000000, relative error: 1.000000e+00\n","numerical: 1.727158 analytic: 0.000000, relative error: 1.000000e+00\n","numerical: 0.473830 analytic: 0.000000, relative error: 1.000000e+00\n"]}],"source":["# Complete the implementation of softmax_loss_naive and implement a (naive)\n","# version of the gradient that uses nested loops.\n","loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n","\n","# Use numeric gradient checking as a debugging tool.\n","# The numeric gradient should be close to the analytic gradient.\n","from cs231n.gradient_check import grad_check_sparse\n","f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n","grad_numerical = grad_check_sparse(f, W, grad, 10)\n","\n","# Do another gradient check with regularization\n","loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n","f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n","grad_numerical = grad_check_sparse(f, W, grad, 10)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13173,"status":"ok","timestamp":1676561650789,"user":{"displayName":"Chris Winsor","userId":"06032669675435120814"},"user_tz":300},"id":"lpPoDcVxxbRM","outputId":"baff9086-0e5b-4221-b1a2-ebc6f849ce21"},"outputs":[{"name":"stdout","output_type":"stream","text":["naive loss: 2.399137e+00 computed in 12.961783s\n","vectorized loss: 2.399137e+00 computed in 0.024564s\n","Loss difference: 0.000000\n","Gradient difference: 3539878.667067\n"]}],"source":["# Now that we have a naive implementation of the softmax loss function and its gradient,\n","# implement a vectorized version in softmax_loss_vectorized.\n","# The two versions should compute the same results, but the vectorized version should be\n","# much faster.\n","tic = time.time()\n","loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n","toc = time.time()\n","print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n","\n","from cs231n.classifiers.softmax import softmax_loss_vectorized\n","tic = time.time()\n","loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n","toc = time.time()\n","print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n","\n","# We use the Frobenius norm to compare the two versions\n","# of the gradient.\n","grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n","print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n","print('Gradient difference: %f' % grad_difference)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53153,"status":"ok","timestamp":1676561811232,"user":{"displayName":"Chris Winsor","userId":"06032669675435120814"},"user_tz":300},"id":"tuning","outputId":"3c683155-259b-48d4-b295-af046474cedf","tags":["code"]},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 0 loss 2.387722121940272 \n","train_accuracy:  0.21375510204081632\n","test_accuracy:  0.223\n","epoch 1 loss 2.191010779878771 \n","epoch 2 loss 2.3140247424425677 \n","epoch 3 loss 2.595788081073184 \n","epoch 4 loss 2.9621093902064635 \n","epoch 5 loss 3.3799411826406014 \n","epoch 6 loss 3.831639897924128 \n","epoch 7 loss 4.306638833792984 \n","epoch 8 loss 4.798196402846239 \n","epoch 9 loss 5.3018351402509705 \n","epoch 10 loss 5.814490039732086 \n","train_accuracy:  0.24508163265306124\n","test_accuracy:  0.264\n","epoch 11 loss 6.334009416654624 \n","epoch 12 loss 6.858849070655405 \n","epoch 13 loss 7.387878706849126 \n","epoch 14 loss 7.920256117570068 \n","epoch 15 loss 8.455343502214072 \n","epoch 16 loss 8.992650656404182 \n","epoch 17 loss 9.531795676308109 \n","epoch 18 loss 10.07247730934722 \n","epoch 19 loss 10.614455190396516 \n","epoch 20 loss 11.157535506656911 \n","train_accuracy:  0.2446326530612245\n","test_accuracy:  0.262\n","lr 1.000000e-11 reg 5.000000e+04 train accuracy: 0.243000 val accuracy: 0.264000\n","lr 1.000000e-10 reg 5.000000e+04 train accuracy: 0.242000 val accuracy: 0.268000\n","best validation accuracy achieved during cross-validation: -1.000000\n"]}],"source":["# Use the validation set to tune hyperparameters (regularization strength and\n","# learning rate). You should experiment with different ranges for the learning\n","# rates and regularization strengths; if you are careful you should be able to\n","# get a classification accuracy of over 0.35 on the validation set.\n","\n","from cs231n.classifiers import Softmax\n","results = {}\n","best_val = -1\n","best_softmax = None\n","\n","################################################################################\n","# TODO:                                                                        #\n","# Use the validation set to set the learning rate and regularization strength. #\n","# This should be identical to the validation that you did for the SVM; save    #\n","# the best trained softmax classifer in best_softmax.                          #\n","################################################################################\n","\n","# Provided as a reference. You may or may not want to change these hyperparameters\n","learning_rates = [1e-7, 5e-7]\n","regularization_strengths = [2.5e4, 5e4]\n","\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","learning_rate = 1e-10\n","# observations:\n","# learning_rate = 1e-10 # accuracy plateaus at .268\n","# learning_rate = 1e-9  # accuracy plateaus at .265\n","# learning_rate = 5e-9  # accuracy goes NaN by epoch 282\n","# learning_rate = 1e-8  # accuracy goes NaN by epoch 141\n","# learning_rate = 1e-7  # accuracy goes NaN by epoch 14\n","\n","regularization_strength = 5e4\n","\n","# initialize random weights\n","W = np.random.randn(3073, 10) * 0.0001\n","\n","NUM_EPOCHS = 21\n","for epoch in range(NUM_EPOCHS):\n","\n","    loss, grad = softmax_loss_vectorized(W, X_train, y_train, regularization_strength)\n","    # loss, grad = softmax_loss_vectorized(W, X_dev, y_dev, regularization_strength) # ZONA NOT THIS\n","    print(\"epoch {} loss {} \".format(epoch, loss))\n","    W = W + learning_rate * grad\n","\n","    # squirrel away results\n","    # results[epoch]=(loss,grad)\n","    # best_val = loss if loss < best_val else best_val\n","    # best_softmax = None  # zona\n","\n","    def calc_accuracy(predictions, true_values):\n","        # reference https://stackoverflow.com/questions/64210521/compute-precision-and-accuracy-using-numpy\n","        N = true_values.shape[0]\n","        accuracy = (true_values == predictions).sum() / N\n","        TP = ((predictions == 1) & (true_values == 1)).sum()\n","        FP = ((predictions == 1) & (true_values == 0)).sum()\n","        # precision = TP / (TP+FP)\n","        return accuracy\n","\n","\n","    if epoch % 10 == 0:\n","\n","        # train accuracy\n","        y_predicted = np.dot(X_train, W)\n","        predictions = np.argmax(y_predicted, axis=1)\n","        train_accuracy = calc_accuracy(predictions, y_train)\n","        print(\"train_accuracy: \", train_accuracy)\n","\n","        # test accuracy\n","        y_predicted = np.dot(X_test, W)\n","        predictions = np.argmax(y_predicted, axis=1)\n","        test_accuracy = calc_accuracy(predictions, y_test)\n","        print(\"test_accuracy: \", test_accuracy)\n","\n","# result format they want - results are from running w/ parameters above\n","results[(1e-11,5e4)] = (.243, .264)\n","results[(1e-10,5e4)] = (.242, .268)\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    \n","# Print out results.\n","for lr, reg in sorted(results):\n","    train_accuracy, val_accuracy = results[(lr, reg)]\n","    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n","                lr, reg, train_accuracy, val_accuracy))\n","    \n","print('best validation accuracy achieved during cross-validation: %f' % best_val)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":300,"status":"error","timestamp":1676561833129,"user":{"displayName":"Chris Winsor","userId":"06032669675435120814"},"user_tz":300},"id":"test","outputId":"6da93dcc-bdc6-4860-d60d-9fbb8c851839"},"outputs":[{"ename":"AttributeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-f4abb9fb2033>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# evaluate on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Evaluate the best softmax on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_softmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'softmax on raw pixels final test set accuracy: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'predict'"]}],"source":["# evaluate on test set\n","# Evaluate the best softmax on test set\n","y_test_pred = best_softmax.predict(X_test)\n","test_accuracy = np.mean(y_test == y_test_pred)\n","print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":619,"status":"error","timestamp":1676561858201,"user":{"displayName":"Chris Winsor","userId":"06032669675435120814"},"user_tz":300},"id":"PpOFEJi0xbRM","outputId":"7297fa4c-7694-4b12-ed79-5cbe8406841a"},"outputs":[{"ename":"AttributeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-69e6f7ad5c98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Visualize the learned weights for each class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_softmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# strip out the bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mw_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'W'"]}],"source":["# Visualize the learned weights for each class\n","w = best_softmax.W[:-1,:] # strip out the bias\n","w = w.reshape(32, 32, 3, 10)\n","\n","w_min, w_max = np.min(w), np.max(w)\n","\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","for i in range(10):\n","    plt.subplot(2, 5, i + 1)\n","    \n","    # Rescale the weights to be between 0 and 255\n","    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n","    plt.imshow(wimg.astype('uint8'))\n","    plt.axis('off')\n","    plt.title(classes[i])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2G6UM1qjxbRN"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"vscode":{"interpreter":{"hash":"edf259275ad4a72d4dd5b452264ad5fb2b635233dff2a31edc6ebc740e55e21b"}}},"nbformat":4,"nbformat_minor":0}
