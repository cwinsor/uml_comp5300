{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EkV5i3_TYUIw"
      },
      "outputs": [],
      "source": [
        "# cloneright 2020 The HuggingFace Team. All rights reserved.\n",
        "# Copyright 2022 Vladislav Lialin\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4cRE8IbIrIV"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to\n",
        "\n",
        "1. install ðŸ¤— Transformers and ðŸ¤— Datasets. Uncomment the following cell and run it.\n",
        "2. make sure your runtime is GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (2.10.1)\n",
            "Requirement already satisfied: transformers in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (4.27.1)\n",
            "Requirement already satisfied: wandb in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (0.14.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from datasets) (1.23.4)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from datasets) (2022.3.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from datasets) (8.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from datasets) (0.13.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: pandas in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from datasets) (1.5.1)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: xxhash in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from datasets) (0.0.0)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from datasets) (2.28.1)\n",
            "Requirement already satisfied: responses<0.19 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: filelock in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from wandb) (3.1.31)\n",
            "Requirement already satisfied: setproctitle in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from wandb) (1.16.0)\n",
            "Requirement already satisfied: pathtools in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: setuptools in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from wandb) (65.5.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from wandb) (5.9.4)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from wandb) (4.4.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from Click!=8.0.0,>=7.0->wandb) (0.4.4)\n",
            "Requirement already satisfied: six>=1.4.0 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.13)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (3.0.5)\n"
          ]
        }
      ],
      "source": [
        "! pip install datasets transformers wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SfkbFABAYgms"
      },
      "outputs": [],
      "source": [
        "# Uncomment this to verify that you have a GPU\n",
        "# !nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V29soEZFYUJD"
      },
      "source": [
        "Make sure your version of Transformers is at least 4.11.0 since the functionality was introduced in that version:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nUpYMxzRYUJE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.27.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import transformers\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Fine-tuning a model on a text classification task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTCFado4IrIc"
      },
      "source": [
        "In this notebook, we will see how to fine-tune one of the [ðŸ¤— Transformers](https://github.com/huggingface/transformers) model to a text classification task of the [GLUE Benchmark](https://gluebenchmark.com/).\n",
        "\n",
        "The GLUE Benchmark is a group of nine classification tasks on sentences or pairs of sentences which are:\n",
        "\n",
        "- [CoLA](https://nyu-mll.github.io/CoLA/) (Corpus of Linguistic Acceptability) Determine if a sentence is grammatically correct or not.is a  dataset containing sentences labeled grammatically correct or not.\n",
        "- [MNLI](https://arxiv.org/abs/1704.05426) (Multi-Genre Natural Language Inference) Determine if a sentence entails, contradicts or is unrelated to a given hypothesis. (This dataset has two versions, one with the validation and test set coming from the same distribution, another called mismatched where the validation and test use out-of-domain data.)\n",
        "- [MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398) (Microsoft Research Paraphrase Corpus) Determine if two sentences are paraphrases from one another or not.\n",
        "- [QNLI](https://rajpurkar.github.io/SQuAD-explorer/) (Question-answering Natural Language Inference) Determine if the answer to a question is in the second sentence or not. (This dataset is built from the SQuAD dataset.)\n",
        "- [QQP](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs) (Quora Question Pairs2) Determine if two questions are semantically equivalent or not.\n",
        "- [RTE](https://aclweb.org/aclwiki/Recognizing_Textual_Entailment) (Recognizing Textual Entailment) Determine if a sentence entails a given hypothesis or not.\n",
        "- [SST-2](https://nlp.stanford.edu/sentiment/index.html) (Stanford Sentiment Treebank) Determine if the sentence has a positive or negative sentiment.\n",
        "- [WNLI](https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html) (Winograd Natural Language Inference) Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed or not. (This dataset is built from the Winograd Schema Challenge dataset.)\n",
        "\n",
        "> We do not use STSB here, because it uses slightly different evaluation schema than the rest and we don't want to complicate code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YZbiBDuGIrId"
      },
      "outputs": [],
      "source": [
        "GLUE_TASKS = [\"cola\", \"mnli\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"wnli\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zVvslsfMIrIh"
      },
      "outputs": [],
      "source": [
        "# Task 1.1: Select a task (from GLUE_TASKS) and a model\n",
        "# We recommend \"cola\" and \"bert-base-uncased\"\n",
        "# You can also try \"bert-large-uncased\" (a larger model) and \"distilbert-base-cased\" (a smaller model)\n",
        "# Full list of models: https://huggingface.co/transformers/pretrained_models.html (not all of them will work with this script)\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "task = \"cola\"\n",
        "model_name = \"bert-base-uncased\"\n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "assert task in GLUE_TASKS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7QYTpxXIrIl"
      },
      "source": [
        "We will use the [ðŸ¤— Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions `load_dataset` and `load_metric`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IreSlFmlIrIm"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKx2zKs5IrIq"
      },
      "source": [
        "We can directly pass our task name to those functions. `load_dataset` will cache the dataset to avoid downloading it again the next time you run this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "s_AY1ATSIrIq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset glue (C:/Users/chris/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b79a53d128943babd9f7ded766e77b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_8664\\1378297502.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"glue\", task)\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"glue\", task)\n",
        "metric = load_metric(\"glue\", task)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "datasets_modules.metrics.glue.91f3cfc5498873918ecf119dbf806fb10815786c84f41b85a5d3c47c1519b343.glue.Glue"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# zona delete me\n",
        "type(metric)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzfPtOMoIrIu"
      },
      "source": [
        "The `dataset` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set (with more keys for the mismatched validation and test set in the special case of `mnli`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GWiVUF0jIrIv"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence', 'label', 'idx'],\n",
              "        num_rows: 8551\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['sentence', 'label', 'idx'],\n",
              "        num_rows: 1043\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence', 'label', 'idx'],\n",
              "        num_rows: 1063\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['The professor talked us.',\n",
              " 'We yelled ourselves hoarse.',\n",
              " 'We yelled ourselves.',\n",
              " 'We yelled Harry hoarse.',\n",
              " 'Harry coughed himself into a fit.',\n",
              " 'Harry coughed himself.',\n",
              " 'Harry coughed us into a fit.',\n",
              " 'Bill followed the road into the forest.',\n",
              " 'We drove Highway 5 from SD to SF.',\n",
              " 'Fred tracked the leak to its source.']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"train\"][\"sentence\"][20:30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 1, 0, 0, 1, 0, 0, 1, 1, 1]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"train\"][\"label\"][20:30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random\n",
        "random.randint(0,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Pe4NNJepYUJK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'sentence': 'There was found in this cave an ancient treasure trove.', 'label': 1, 'idx': 2455}\n",
            "{'sentence': 'Pat handed to Chris.', 'label': 0, 'idx': 4618}\n",
            "{'sentence': 'Bill broke the bathtub.', 'label': 1, 'idx': 16}\n",
            "{'sentence': 'It was to finish the homework that John tried.', 'label': 0, 'idx': 5081}\n",
            "{'sentence': \"I didn't realize that it had rained and some crops had been destroyed.\", 'label': 1, 'idx': 1945}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[None, None, None, None, None]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Task 1.1: To get a sense of what the data looks like, print 5 random examples from the \"train\" subset\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "num_samples = len(dataset[\"train\"])\n",
        "[print(dataset[\"train\"][random.randint(0,num_samples)]) for _ in range(0,5)]\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnjDIuQ3IrI-"
      },
      "source": [
        "The metric is an instance of [`datasets.Metric`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Metric):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5o4rUteaIrI_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Metric(name: \"glue\", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: \"\"\"\n",
              "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
              "Args:\n",
              "    predictions: list of predictions to score.\n",
              "        Each translation should be tokenized into a list of tokens.\n",
              "    references: list of lists of references for each translation.\n",
              "        Each reference should be tokenized into a list of tokens.\n",
              "Returns: depending on the GLUE subset, one or several of:\n",
              "    \"accuracy\": Accuracy\n",
              "    \"f1\": F1 score\n",
              "    \"pearson\": Pearson Correlation\n",
              "    \"spearmanr\": Spearman Correlation\n",
              "    \"matthews_correlation\": Matthew Correlation\n",
              "Examples:\n",
              "\n",
              "    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
              "    >>> references = [0, 1]\n",
              "    >>> predictions = [0, 1]\n",
              "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
              "    >>> print(results)\n",
              "    {'accuracy': 1.0}\n",
              "\n",
              "    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
              "    >>> references = [0, 1]\n",
              "    >>> predictions = [0, 1]\n",
              "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
              "    >>> print(results)\n",
              "    {'accuracy': 1.0, 'f1': 1.0}\n",
              "\n",
              "    >>> glue_metric = datasets.load_metric('glue', 'stsb')\n",
              "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
              "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
              "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
              "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
              "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
              "\n",
              "    >>> glue_metric = datasets.load_metric('glue', 'cola')\n",
              "    >>> references = [0, 1]\n",
              "    >>> predictions = [0, 1]\n",
              "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
              "    >>> print(results)\n",
              "    {'matthews_correlation': 1.0}\n",
              "\"\"\", stored examples: 0)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAWdqcUBIrJC"
      },
      "source": [
        "You can call its `compute` method with your predictions and labels directly and it will return a dictionary with the metric(s) value:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6XN1Rq0aIrJC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'matthews_correlation': 0.29277002188455997}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Task 1.2: Compute the metric using `fake_preds` and `fake_labels`\n",
        "# YOUR CODE STARTS HERE\n",
        "# feel free to modify fake_preds and fake_labels if your metric does not work for these inputs\n",
        "fake_preds = np.array([1, 2, 0, 0, 0, 1, 1, 2])\n",
        "fake_labels = np.array([1, 1, 0, 2, 0, 2, 2, 2])\n",
        "\n",
        "metric_value = metric.compute(predictions=fake_preds, references=fake_labels)\n",
        "print(metric_value)\n",
        "# YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOCrQwPoIrJG"
      },
      "source": [
        "Note that `load_metric` has loaded the proper metric associated to your task, which is:\n",
        "\n",
        "- for CoLA: [Matthews Correlation Coefficient](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient)\n",
        "- for MNLI (matched or mismatched): Accuracy\n",
        "- for MRPC: Accuracy and [F1 score](https://en.wikipedia.org/wiki/F1_score)\n",
        "- for QNLI: Accuracy\n",
        "- for QQP: Accuracy and [F1 score](https://en.wikipedia.org/wiki/F1_score)\n",
        "- for RTE: Accuracy\n",
        "- for SST-2: Accuracy\n",
        "- for WNLI: Accuracy\n",
        "\n",
        "so the metric object only computes the one(s) needed for your task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVx71GdAIrJH"
      },
      "source": [
        "Before we can feed those texts to our model, we need to preprocess them. This is done by a ðŸ¤— Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
        "\n",
        "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
        "\n",
        "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
        "- we download the vocabulary used when pretraining this specific checkpoint.\n",
        "\n",
        "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Task 1.3: Load a pre-trained tokenizer.\n",
        "# Remember that we want to work with a pre-trained model,\n",
        "# which means that we need to use exactly the same tokenizer that was used to pre-train the model.\n",
        "# Provide your model name to the .from_pretrained method and it will download the right tokenizer files for you.\n",
        "# Provide `use_fast=True` to use the fast tokenizer.\n",
        "# You can learn more about fast tokenizers vs slow tokenizers here: https://www.youtube.com/watch?v=g8quOxoqhHQ\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "print(type(tokenizer))\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl6IidfdIrJK"
      },
      "source": [
        "We pass along `use_fast=True` to the call above to use one of the fast tokenizers (backed by Rust) from the ðŸ¤— Tokenizers library. Those fast tokenizers are available for almost all models, but if you got an error with the previous call, remove that argument."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rowT4iCLIrJK"
      },
      "source": [
        "You can directly call this tokenizer on one sentence or a pair of sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "a5hBlsrHIrJL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7592, 1010, 2023, 2028, 6251, 999, 102, 1998, 2023, 6251, 3632, 2007, 2009, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(\"Hello, this one sentence!\", text_pair=\"And this sentence goes with it.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo_0B1M2IrJM"
      },
      "source": [
        "Depending on the model you selected, you will see different keys in the dictionary returned by the cell above. They don't matter much for what we're doing here (just know they are required by the model we will instantiate later), you can learn more about them in [this tutorial](https://huggingface.co/transformers/preprocessing.html) if you're interested.\n",
        "\n",
        "To preprocess our dataset, we will thus need the names of the columns containing the sentence(s). The following dictionary keeps track of the correspondence task to column names:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fyGdtK9oIrJM"
      },
      "outputs": [],
      "source": [
        "task_to_keys = {\n",
        "    \"cola\": (\"sentence\", None),\n",
        "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
        "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
        "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
        "    \"qnli\": (\"question\", \"sentence\"),\n",
        "    \"qqp\": (\"question1\", \"question2\"),\n",
        "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
        "    \"sst2\": (\"sentence\", None),\n",
        "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
        "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbqtC4MrIrJO"
      },
      "source": [
        "We can double check it does work on our current dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "19GG646uIrJO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence: Our friends won't buy this analysis, let alone the next one we propose.\n"
          ]
        }
      ],
      "source": [
        "sentence1_key, sentence2_key = task_to_keys[task]\n",
        "if sentence2_key is None:\n",
        "    print(f\"Sentence: {dataset['train'][0][sentence1_key]}\")\n",
        "else:\n",
        "    print(f\"Sentence 1: {dataset['train'][0][sentence1_key]}\")\n",
        "    print(f\"Sentence 2: {dataset['train'][0][sentence2_key]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C0hcmp9IrJQ"
      },
      "source": [
        "We can them write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True`. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vc0BSBLIIrJQ"
      },
      "outputs": [],
      "source": [
        "# Task 1.4: Implement preprocess_function that applies tokenizer to either a single sentence or to a pair of sentences.\n",
        "# Truncate the sequences via passing `truncation=True` to the tokenizer in case some texts have larger length that the model can process (512 tokens).\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "def preprocess_function(sample):\n",
        "    sentence1_key, sentence2_key = task_to_keys[task]\n",
        "    if sentence2_key is None:\n",
        "        # Tokenize a single sentence\n",
        "        tokenized = tokenizer(sample[sentence1_key], truncation=True)\n",
        "        return tokenized\n",
        "\n",
        "    # Tokenize a pair of sentences: one defined by sentence1_key, the other defined by sentence2_key\n",
        "    tokenized = [tokenizer(sample[sentence1_key], truncation=True),\n",
        "                 tokenizer(sample[sentence2_key], truncation=True)]\n",
        "    return tokenized\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lm8ozrJIrJR"
      },
      "source": [
        "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-b70jh26IrJS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset glue (C:/Users/chris/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43b04b8b1db646b7af457460334c819f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding(num_tokens=19, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "Encoding(num_tokens=14, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "Encoding(num_tokens=14, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "Encoding(num_tokens=15, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "Encoding(num_tokens=13, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
          ]
        }
      ],
      "source": [
        "# Task 1.5 to make sure your preprocess_function works, apply it to the first 5 elements of the train set\n",
        "# You can get them vias dataset[\"train\"][:5]\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "dataset = load_dataset(\"glue\", task)\n",
        "metric = load_metric(\"glue\", task)\n",
        "y = preprocess_function(dataset[\"train\"][:5])\n",
        "for i in range(0,5):\n",
        "    print(y[i])\n",
        "# [print(yy) for yy in y]\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS-6iXTkIrJT"
      },
      "source": [
        "To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command.\n",
        "\n",
        "Because you apply this fuction to the `dataset` object that contains all `train`, `valid` and `test` subsets,\n",
        "encoded_dataset will have all of the same subsets.\n",
        "use `remove_columns` to remove all of the columns that are not needed for the model\n",
        "(all `dataset[\"train\"].column_names`) except `label` (we have already created one for you -- `old_column_names`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['sentence', 'label', 'idx']"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"train\"].column_names \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\glue\\cola\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-90ab52738119f63d.arrow\n",
            "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\glue\\cola\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-e25344e3eab6a314.arrow\n",
            "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\glue\\cola\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-b0a635170a1d207e.arrow\n"
          ]
        }
      ],
      "source": [
        "encoded_dataset = dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask']"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_dataset[\"train\"].column_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['sentence', 'idx']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "old_column_names = [c for c in dataset[\"train\"].column_names if c != \"label\"]\n",
        "old_column_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "DDtsaJeVIrJT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\glue\\cola\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-90ab52738119f63d.arrow\n",
            "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\glue\\cola\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-e25344e3eab6a314.arrow\n",
            "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\glue\\cola\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-b0a635170a1d207e.arrow\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "after: ['label', 'input_ids', 'token_type_ids', 'attention_mask']\n"
          ]
        }
      ],
      "source": [
        "old_column_names = [c for c in dataset[\"train\"].column_names if c != \"label\"]\n",
        "\n",
        "# Task 1.6: map the preprocess_function to the dataset\n",
        "# Use batched=True, so it would run faster\n",
        "# (batched operations are always faster than a single-element ones, even though they are a bit harder to write)\n",
        "# Assign the results to encoded_dataset\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
        "# print(f'before: {encoded_dataset[\"train\"].column_names}')\n",
        "# print(f'old_columnn_names {old_column_names}')\n",
        "for c in old_column_names:\n",
        "    encoded_dataset = encoded_dataset.remove_columns(c)\n",
        "# encoded_dataset = dataset.rename_column(\"idx\", \"input_ids\")\n",
        "print(f'after: {encoded_dataset[\"train\"].column_names}')\n",
        "\n",
        "# YOUR CODE ENDS HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "oOYLqG0fYUJR"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'label': 1,\n",
              " 'input_ids': [101,\n",
              "  2256,\n",
              "  2814,\n",
              "  2180,\n",
              "  1005,\n",
              "  1056,\n",
              "  4965,\n",
              "  2023,\n",
              "  4106,\n",
              "  1010,\n",
              "  2292,\n",
              "  2894,\n",
              "  1996,\n",
              "  2279,\n",
              "  2028,\n",
              "  2057,\n",
              "  16599,\n",
              "  1012,\n",
              "  102],\n",
              " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voWiw8C7IrJV"
      },
      "source": [
        "The `.map` results are automatically cached by the ðŸ¤— Datasets library to avoid spending time on this step the next time you run your notebook. The ðŸ¤— Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). For instance, it will properly detect if you change the task in the first cell and rerun the notebook. ðŸ¤— Datasets warns you when it uses cached files, you can pass `load_from_cache_file=False` in the call to `map` to not use the cached files and force the preprocessing to be applied again.\n",
        "\n",
        "Note that we passed `batched=True` to encode the texts by batches together. This is to leverage the full benefit of the fast tokenizer we loaded earlier, which will use multi-threading to treat the texts in a batch concurrently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBiW8UpKIrJW"
      },
      "source": [
        "Now that our data is ready, we can download the pretrained model and fine-tune it. Since all our tasks are about sentence classification, we use the `AutoModel` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us. This model will return us the hidden states of the pre-trained transformers. We need to take them and use to predict the classes. To do that we need to pool them across the time dimension (to get a fixed-size vector independent on the sequence length). The most common way to do that is to wrap the model into a new `nn.Module` class that will have our pre-trained transformer and an extra linear layer (feel free to use more than one layer).\n",
        "\n",
        "But first, let's load the model using `AutoModel.from_pretrained` and play with it to learn its interfaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "TlqNaB8jIrJW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "# Task 1.7: Load a pre-trained model using the same name you used for the tokenizer\n",
        "# name it bert_model\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "bert_model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7BVl5vSYUJS"
      },
      "source": [
        "To tokenize the sentence (or a pair of sentences), we use a tokenizer.\n",
        "More that that, we can ask the tokenizer to return us pytorch tensors if we provide `return_tensors=\"pt\"`, where \"pt\" means PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "WXSfLOEqYUJS"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 7592, 1010, 2023, 2003, 1996, 6251, 2193, 2028,  999,  102, 2023,\n",
              "         2003, 2178, 6251,  999,  102,    0,    0,    0,    0],\n",
              "        [ 101, 1998, 2023, 6251, 3632, 2007, 1996, 2034, 6251, 1012,  102, 1998,\n",
              "         2023, 6251, 3632, 2007, 1996, 2117, 2028, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts = [\"Hello, this is the sentence number one!\",\n",
        "         \"This is another sentence!\"]\n",
        "text_pairs = [\"And this sentence goes with the first sentence.\",\n",
        "              \"And this sentence goes with the second one.\"]\n",
        "\n",
        "# Task 1.8: Tokenize the batch of texts and text pairs\n",
        "# Note that you need to make the tokenizer to\n",
        "# 1. return pytorch tensors\n",
        "# 2. do padding\n",
        "# If padding is not turned on, the model won't be able to create a batch of these two examples.\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "batch = [texts, text_pairs]\n",
        "input_object = tokenizer(batch, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "input_object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sZf3P2eYUJT"
      },
      "source": [
        "Your `input_object` should look like this:\n",
        "\n",
        "```python\n",
        "{\n",
        "'input_ids': tensor(\n",
        "        [[ 101, 7592, 1010, 2023, 2003, 1996, 6251, 2193, 2028,  999,  102, 1998, 2023, 6251, 3632, 2007, 1996, 2034, 6251, 1012,  102],\n",
        "        [ 101, 2023, 2003, 2178, 6251,  999,  102, 1998, 2023, 6251, 3632, 2007, 1996, 2117, 2028, 1012,  102,    0,    0,    0,    0]]), \n",
        "'token_type_ids': tensor(\n",
        "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]),\n",
        "'attention_mask': tensor(\n",
        "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdbnD1heYUJT"
      },
      "source": [
        "We can see that a tokenizer returns a dictionary with the following keys:\n",
        "    \n",
        "    - \"input_ids\": a list of token ids\n",
        "    - \"attention_mask\": a list of booleans, where 1 means that the corresponding token is part of the input, and 0 means that it is padding\n",
        "    - \"token_type_ids\": a list of integers, where 0 means that the corresponding token is the first sentence and 1 means that the corresponding token is the second sentence. These are very imporant for tasks like MNLI or SST.\n",
        "    \n",
        "The model expects the arguments with the same keys, so we can forward it with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "cZRbrfZKYUJT"
      },
      "outputs": [],
      "source": [
        "ouput_obj = bert_model(\n",
        "    input_ids=input_object[\"input_ids\"],\n",
        "    attention_mask=input_object[\"attention_mask\"],\n",
        "    token_type_ids=input_object[\"token_type_ids\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ko8KbUugYUJU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "last_last_hidden_state <class 'torch.Tensor'> torch.Size([2, 21, 768])\n",
            "pooler_output <class 'torch.Tensor'> torch.Size([2, 768])\n"
          ]
        }
      ],
      "source": [
        "# Now, look at ouput_obj and figure out how to get the hidden states of the last transformer layer.\n",
        "# Task 1.9: Get the hidden states of the last transformer layer\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "# print(type(bert_model))\n",
        "# print(type(ouput_obj))\n",
        "# assert False, \"hold up\"\n",
        "\n",
        "last_hidden_state = ouput_obj.last_hidden_state\n",
        "pooler_output = ouput_obj.pooler_output\n",
        "print(f\"last_last_hidden_state {type(last_hidden_state)} {last_hidden_state.shape}\")\n",
        "print(f\"pooler_output {type(pooler_output)} {pooler_output.shape}\")\n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "\n",
        "_batch_size = len(input_object[\"input_ids\"])\n",
        "_seq_len = len(input_object[\"input_ids\"][0])\n",
        "_hidden = bert_model.config.hidden_size\n",
        "assert last_hidden_state.shape == (_batch_size, _seq_len, _hidden)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdjq7eFJYUJU"
      },
      "source": [
        "Now it's time to wrap the model into PyTorch `nn.Module`.\n",
        "Your task is to:\n",
        "\n",
        "1. Create BertForClassification class that inherits from `nn.Module`\n",
        "2. Implement `__init__` which should take `pre_trained_encoder` (an object of type `BertModel`) and `num_classes` (integer). Then, as usual, you assign the layer objects (and create new ones, if nesessary) to the object arguments (e.g., `self.pre_trained_encoder` and `self.output_layer`).\n",
        "3. Implement `forward()`. Remember to pool the sequence dimension via extracting the first token hidden (it corresponds to the CLS token).\n",
        "\n",
        "This model should take `input_ids`, `attention_mask`, and `token_type_ids` and to output the logits of the `num_classes`.\n",
        "\n",
        "> DO NOT hard-code any values (e.g., do not assume that model hidden is always equal to 768). Extract this number from the model config as we did in the previuos cell.\n",
        "\n",
        "\n",
        "**Extra points:** Feel free to have more than one layer after the pre_trained_encoder and/or to use Batch Normalization, Dropout, and other tricks. Remember that you need nonlinearities between linear layers, but not after the last layer. You can additionally use different learning rates for the body and the head of the model. You can learn more about common tricks in [fast.ai Lesson 1](https://course.fast.ai/videos/?lesson=1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "9dDn5347YUJU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Passed!\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "class BertForClassification(nn.Module):\n",
        "    def __init__(self, pre_trained_encoder, num_classes):\n",
        "        super(BertForClassification, self).__init__()\n",
        "\n",
        "        self.pre_trained_encoder = pre_trained_encoder\n",
        "        # print(type(self.pre_trained_encoder))\n",
        "\n",
        "        self.bert_out_size = pre_trained_encoder.config.hidden_size\n",
        "\n",
        "        self.lin1 = nn.Linear(self.bert_out_size, self.bert_out_size)\n",
        "        self.relu1 = nn.ReLU(self.bert_out_size)\n",
        "        self.lin2 = nn.Linear(self.bert_out_size, num_classes)\n",
        "        self.relu2 = nn.ReLU(self.bert_out_size)\n",
        "\n",
        "        # self.output_layer = nn.Sequential(\n",
        "        #     nn.ReLU(self.bert_out_size),\n",
        "        #     nn.Linear(self.bert_out_size, self.bert_out_size),\n",
        "        #     nn.ReLU(self.bert_out_size),\n",
        "        #     # nn.BatchNorm1d(self.bert_out_size),\n",
        "        #     # nn.Dropout(0.3),\n",
        "        #     nn.Linear(self.bert_out_size, num_classes),\n",
        "        #     # nn.Sigmoid()\n",
        "        # )\n",
        "\n",
        "        # # Freeze the BERT model\n",
        "        # for param in self.pre_trained_encoder.parameters():\n",
        "        #     param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        x = self.pre_trained_encoder(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        # # last_hidden_state = x.last_hidden_state\n",
        "        # # pooler_output = x1.pooler_output\n",
        "        # pooler_out = x1[\"pooler_output\"].clone()\n",
        "        # x2 = self.output_layer(pooler_out)\n",
        "        # return x2\n",
        "\n",
        "        x= nn.Linear(x)\n",
        "        x= nn.ReLU(x)\n",
        "        x = nn.Linear(x)\n",
        "        x = nn.ReLU(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "\n",
        "bert_model = AutoModel.from_pretrained(model_name)\n",
        "model = BertForClassification(pre_trained_encoder=bert_model, num_classes=17)\n",
        "output_logits = model(input_object[\"input_ids\"], input_object[\"attention_mask\"], input_object[\"token_type_ids\"])\n",
        "assert output_logits.shape == (2, 17)\n",
        "print(\"Passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CczA5lJlIrJX"
      },
      "source": [
        "The warning is telling us we are throwing away some weights. This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "KjeeDN55YUJV"
      },
      "outputs": [],
      "source": [
        "num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2\n",
        "metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\n",
        "\n",
        "# Feel free to change the parameters below\n",
        "# YOUR CODE STARTS HERE\n",
        "# learning_rate = 2e-5\n",
        "learning_rate = 2e-4\n",
        "batch_size = 16\n",
        "num_train_epochs = 50\n",
        "weight_decay = 0.01\n",
        "# YOUR CODE ENDS HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N8urzhyIrJY"
      },
      "source": [
        "Now, it's time to create the dataloaders that will batch the data from our datasets.\n",
        "\n",
        "To do that, you need a collation function that combines multiplle examples (of different lengths in general) into a batch. In machine translation homework we had written our own collation function and it looked like this:\n",
        "\n",
        "```python\n",
        "def collation_function_for_seq2seq(batch, source_pad_token_id, target_pad_token_id):\n",
        "    input_ids_list = [ex[\"input_ids\"] for ex in batch]\n",
        "    decoder_input_ids_list = [ex[\"decoder_input_ids\"] for ex in batch]\n",
        "    labels_list = [ex[\"labels\"] for ex in batch]\n",
        "\n",
        "    collated_batch = {\n",
        "        \"input_ids\": pad(input_ids_list, source_pad_token_id),\n",
        "        \"decoder_input_ids\": pad(decoder_input_ids_list, target_pad_token_id),\n",
        "        \"labels\": pad(labels_list, target_pad_token_id),\n",
        "    }\n",
        "\n",
        "    collated_batch[\"encoder_padding_mask\"] = collated_batch[\"input_ids\"] == source_pad_token_id\n",
        "    return collated_batch\n",
        "\n",
        "def pad(sequence_list, pad_id):\n",
        "    max_len = max(len(x) for x in sequence_list)\n",
        "    padded_sequence_list = []\n",
        "    for sequence in sequence_list:\n",
        "        padding = [pad_id] * (max_len - len(sequence))\n",
        "        padded_sequence = sequence + padding\n",
        "        padded_sequence_list.append(padded_sequence)\n",
        "\n",
        "    return torch.LongTensor(padded_sequence_list)\n",
        "```\n",
        "\n",
        "We can either write a similar function by here or to use `transformers.data.data_collator.DataCollatorWithPadding`.\n",
        "\n",
        "> NOTE: remember to shuffle the order of the dataset elements in your training dataloader (and do not shuffle test dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "EvnbMXPyYUJW"
      },
      "outputs": [],
      "source": [
        "from transformers.data.data_collator import DataCollatorWithPadding\n",
        "\n",
        "# Task 1.10: Create collator, training set dataloader, and validation set dataloader\n",
        "# (these dataloaders should use the collator to collate the examples of different lengths into batches)\n",
        "# YOUR CODE STARTS HERE (our implementation is 3 lines)\n",
        "\n",
        "collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True, return_tensors='pt')\n",
        "train_dataloader = torch.utils.data.DataLoader(encoded_dataset[\"train\"],      batch_size=batch_size, shuffle=True, collate_fn=collator)\n",
        "valid_dataloader = torch.utils.data.DataLoader(encoded_dataset[\"validation\"], batch_size=batch_size, shuffle=True, collate_fn=collator)\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1GdLQKhYUJW"
      },
      "source": [
        "> If dataloader fails, it might happen because it is trying to collate non-collatable objects like strings. This could happend if you forgot to remove the extra columns from the dataset during `.map` a few cells above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([16, 20])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch = next(iter(valid_dataloader))\n",
        "batch[\"input_ids\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "_ToKXfT0YUJW"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  2107,  1037,  6288,  2004,  2017,  2020,  4092,  1997,  2074,\n",
              "          2085,  2003,  2182,  1012,   102,     0,     0,     0,     0,     0,\n",
              "             0,     0],\n",
              "        [  101,  8502,  2295,  1045,  2113,  2195,  3337,  2040,  2024,  1010,\n",
              "          1045,  1005,  1049,  2145,  2183,  2000,  5914, 11458,  1012,   102,\n",
              "             0,     0],\n",
              "        [  101,  5965,  5683,  2986,  1012,   102,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0],\n",
              "        [  101,  1045,  2031,  2070,  4981,  2000,  3046,  2000,  3926, 26886,\n",
              "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0],\n",
              "        [  101,  2296,  3076,  2038,  2000,  2272,  2039,  2007,  2093,  9918,\n",
              "          2008,  2265,  2008,  2070,  4650,  3818,  2011,  3021,  2003,  3308,\n",
              "          1012,   102],\n",
              "        [  101,  1996,  2111,  3407,  2007,  1996,  6378,  2187,  1012,   102,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0],\n",
              "        [  101,  1996,  2060,  2933,  2016,  5837,  2041,  1997,  2192,  1012,\n",
              "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0],\n",
              "        [  101,  3021,  3849,  2000,  2022, 27885,  3630, 25171,  1010,  2021,\n",
              "          1045,  2123,  1005,  1056,  2228,  2008,  3520, 12102,  1012,   102,\n",
              "             0,     0],\n",
              "        [  101,  1045,  2228,  2008,  4083,  2394,  2003,  2025,  2035,  2008,\n",
              "          3733,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0],\n",
              "        [  101,  2057,  2872,  1996,  8808,  1999,  1996, 18097,  1012,   102,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0],\n",
              "        [  101,  1996,  2482,  2097,  2031,  2042,  2108,  5533,  1012,   102,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0],\n",
              "        [  101,  2045,  2003,  2438,  7852,  2005,  2035,  1997,  2017,  1012,\n",
              "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0],\n",
              "        [  101, 11940,  2056,  2008,  2016,  1005,  1040,  3610, 10225,  1998,\n",
              "          5926, 29019,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0],\n",
              "        [  101,  2009,  2001,  2198,  2008,  2001,  3403,  2012,  1996,  4825,\n",
              "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0],\n",
              "        [  101, 11130,  2106,  2025,  2079,  1037,  2067, 11238,  1012,   102,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0],\n",
              "        [  101,  2009,  3849,  2008, 12943, 14074,  2213,  8540,  2187,  1012,\n",
              "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1])}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# make sure this line does not fail and gives you a reasonable output\n",
        "batch = next(iter(train_dataloader))\n",
        "batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8ez_CSkYUJW"
      },
      "source": [
        "Now it's time to build a training loop. It will be similar (but not completely similar) to the one you built in the previous homework.\n",
        "\n",
        "1. iterate over the number of epochs,\n",
        "1. iterate over your training examples in the train_dataloader,\n",
        "1. compute the loss and update model parameters,\n",
        "1. in the end of epoch, compute the metric (using `metric`` object) on the validation set.\n",
        "1. **after all training is done, print the final validation metric**\n",
        "\n",
        "Log your training loss and accuracy and your validtion accuracy to wandb (you will need to submit a link to the most succesfull wandb run).\n",
        "\n",
        "You can use `tqdm` to make a progress bar for your epochs and/or iterations within one epohc (over `train_dataloader`).\n",
        "\n",
        "It is not nesessary, but recommended to wrap evaluation loop into a block\n",
        "\n",
        "```python\n",
        "    with torch.no_grad():\n",
        "        ...\n",
        "```\n",
        "\n",
        "This can make evaluation a bit faster, because pytorch won't track the gradient values. \n",
        "\n",
        "> Remember to put your model into eval mode in the beginning of the evaluation and to put it back into train mode at the end or the model would use dropout during evaluatoin which will **very significantly** affect your results.\n",
        "\n",
        "> If your epoch for CoLA takes **more than 5 minutes**, you are probably not utilizing the GPU (forgot to move your model and data to the correct device).\n",
        "\n",
        "Please refer to [this guide](https://docs.wandb.ai/guides/track/jupyter) on how to work with wandb inside jupyter notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "qcIm3YRrTWwe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random classifier matthews_correlation on cola is {'matthews_correlation': 0.03410118796357716}. Your model should perform **way** better than that.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "# random classifier metric on the eval dataset\n",
        "_labels = encoded_dataset[\"validation\"][\"label\"]\n",
        "_shuffled_labels = _labels.copy()\n",
        "random.shuffle(_shuffled_labels)\n",
        "\n",
        "_random_metric_value = metric.compute(predictions=_shuffled_labels, references=_labels)\n",
        "print(f\"Random classifier {metric_name} on {task} is {_random_metric_value}. Your model should perform **way** better than that.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 0, 0, 0, 1, 1, 1]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_dataset[\"validation\"][\"label\"][0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 0, 0, 0, 1, 1, 1]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_dataset[\"validation\"][\"label\"][0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "AoPVHuMkYUJX"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Bliy8zgjIrJY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcwinsor\u001b[0m (\u001b[33mmetrowest\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.14.2 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>g:\\My Drive\\code_uml\\uml_comp5300\\hw7_long_transformers\\wandb\\run-20230413_193652-ywyy4p7j</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/metrowest/bert_classification_cola/runs/ywyy4p7j' target=\"_blank\">curious-sound-101</a></strong> to <a href='https://wandb.ai/metrowest/bert_classification_cola' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/metrowest/bert_classification_cola' target=\"_blank\">https://wandb.ai/metrowest/bert_classification_cola</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/metrowest/bert_classification_cola/runs/ywyy4p7j' target=\"_blank\">https://wandb.ai/metrowest/bert_classification_cola/runs/ywyy4p7j</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "562c301d0307429086522d91db308a6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mg:\\My Drive\\code_uml\\uml_comp5300\\hw7_long_transformers\\hw6c_bert_classification_unsolved.ipynb Cell 73\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/code_uml/uml_comp5300/hw7_long_transformers/hw6c_bert_classification_unsolved.ipynb#Y306sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m token_type_ids \u001b[39m=\u001b[39m trainbatch[\u001b[39m\"\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/code_uml/uml_comp5300/hw7_long_transformers/hw6c_bert_classification_unsolved.ipynb#Y306sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m reference_labels \u001b[39m=\u001b[39m trainbatch[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/My%20Drive/code_uml/uml_comp5300/hw7_long_transformers/hw6c_bert_classification_unsolved.ipynb#Y306sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids, attention_mask, token_type_ids)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/code_uml/uml_comp5300/hw7_long_transformers/hw6c_bert_classification_unsolved.ipynb#Y306sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m predicted_labels \u001b[39m=\u001b[39m outputs[:,\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mto(device)  \u001b[39m#  0=sequence 1=pooled\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/code_uml/uml_comp5300/hw7_long_transformers/hw6c_bert_classification_unsolved.ipynb#Y306sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# loss_fn = torch.nn.CrossEntropyLoss()\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32mg:\\My Drive\\code_uml\\uml_comp5300\\hw7_long_transformers\\hw6c_bert_classification_unsolved.ipynb Cell 73\u001b[0m in \u001b[0;36mBertForClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/code_uml/uml_comp5300/hw7_long_transformers/hw6c_bert_classification_unsolved.ipynb#Y306sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, attention_mask, token_type_ids):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/My%20Drive/code_uml/uml_comp5300/hw7_long_transformers/hw6c_bert_classification_unsolved.ipynb#Y306sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpre_trained_encoder(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/code_uml/uml_comp5300/hw7_long_transformers/hw6c_bert_classification_unsolved.ipynb#Y306sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39m# last_hidden_state = x.last_hidden_state\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/code_uml/uml_comp5300/hw7_long_transformers/hw6c_bert_classification_unsolved.ipynb#Y306sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39m# pooler_output = x1.pooler_output\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/code_uml/uml_comp5300/hw7_long_transformers/hw6c_bert_classification_unsolved.ipynb#Y306sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     pooler_out \u001b[39m=\u001b[39m x1[\u001b[39m\"\u001b[39m\u001b[39mpooler_output\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mclone()\n",
            "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1011\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1013\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1014\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1015\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1019\u001b[0m )\n\u001b[1;32m-> 1020\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1021\u001b[0m     embedding_output,\n\u001b[0;32m   1022\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1023\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1024\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1025\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1026\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1027\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1028\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1029\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1030\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1031\u001b[0m )\n\u001b[0;32m   1032\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1033\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    601\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    603\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    608\u001b[0m     )\n\u001b[0;32m    609\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    611\u001b[0m         hidden_states,\n\u001b[0;32m    612\u001b[0m         attention_mask,\n\u001b[0;32m    613\u001b[0m         layer_head_mask,\n\u001b[0;32m    614\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    615\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    616\u001b[0m         past_key_value,\n\u001b[0;32m    617\u001b[0m         output_attentions,\n\u001b[0;32m    618\u001b[0m     )\n\u001b[0;32m    620\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    621\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
            "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:537\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    534\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    535\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 537\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    538\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[0;32m    539\u001b[0m )\n\u001b[0;32m    540\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[0;32m    542\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages\\transformers\\pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 248\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
            "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:550\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[0;32m    549\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 550\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[0;32m    551\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
            "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:463\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, input_tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m    462\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(hidden_states)\n\u001b[1;32m--> 463\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(hidden_states)\n\u001b[0;32m    464\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n\u001b[0;32m    465\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
            "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
            "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\pytorch_c_cuda_11_7\\lib\\site-packages\\torch\\nn\\functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[0;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[1;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "run = wandb.init(project=f\"bert_classification_{task}\")\n",
        "\n",
        "# Feel free to modify the code below\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "bert_model = AutoModel.from_pretrained(model_name)\n",
        "model = BertForClassification(pre_trained_encoder=bert_model, num_classes=num_labels)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Task 1.11: Training loop\n",
        "# YOUR CODE STARTS HERE\n",
        "import math\n",
        "import datetime\n",
        "run_name = datetime.datetime.now().strftime(\"%m%d_%H%M%S\")\n",
        "\n",
        "VALIDATION_FREQUENCY = 500\n",
        "best_val_loss = math.inf\n",
        "\n",
        "# ZONA DELETE ME\n",
        "master_batch = next(iter(train_dataloader))\n",
        "\n",
        "# loss_fn = torch.nn.CrossEntropyLoss()\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "global_batch = 0\n",
        "for epoch in tqdm(range(0, num_train_epochs)):\n",
        "    model.train(True)  # put model in training mode\n",
        "    for trainbatch in train_dataloader:\n",
        "        \n",
        "        # trainbatch = master_batch  # ZONA DELETE ME see if it will overfit\n",
        "\n",
        "\n",
        "        global_batch += 1\n",
        "\n",
        "        input_ids = trainbatch[\"input_ids\"].clone().to(device)\n",
        "        attention_mask = trainbatch[\"attention_mask\"].clone().to(device)\n",
        "        token_type_ids = trainbatch[\"token_type_ids\"].clone().to(device)\n",
        "        reference_labels = trainbatch[\"labels\"].clone().float().to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
        "        predicted_labels = outputs[:,1].clone().to(device)  #  0=sequence 1=pooled\n",
        "\n",
        "        # loss_fn = torch.nn.CrossEntropyLoss()\n",
        "        loss = loss_fn(input=predicted_labels, target=reference_labels)\n",
        "        # loss = loss_fn(predicted_labels, reference_labels)\n",
        "\n",
        "\n",
        "        metrics = metric.compute(predictions=predicted_labels, references=reference_labels)\n",
        "\n",
        "\n",
        "        wandb.log(\n",
        "            {\n",
        "                \"training_loss\": loss,\n",
        "                \"training_matthews_correlation\": metrics['matthews_correlation'],\n",
        "                \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
        "                \"epoch\": epoch,\n",
        "            },\n",
        "            step=global_batch,\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        # if global_batch % VALIDATION_FREQUENCY == 0:\n",
        "        #     # zona want this with torch.no_grad():\n",
        "        #     #  zona - want this  model.train(False) model.eval()  # set model to eval mode\n",
        "        #     total_val_metric = 0.\n",
        "        #     total_val_loss = 0.\n",
        "        #     num_val_batches = 0\n",
        "        #     for validbatch in valid_dataloader:\n",
        "        #         num_val_batches += 1\n",
        "\n",
        "        #         validbatch.to(device)\n",
        "        #         val_reference_labels = validbatch[\"labels\"].float().to(device)\n",
        "        #         outputs = model(validbatch[\"input_ids\"], validbatch[\"attention_mask\"], validbatch[\"token_type_ids\"])\n",
        "        #         val_predicted_labels = outputs[:,1].to(device)  #  0=sequence 1=pooled\n",
        "\n",
        "        #         loss_fn = torch.nn.MSELoss()\n",
        "        #         loss = loss_fn(val_predicted_labels, val_reference_labels)\n",
        "\n",
        "        #         metrics = metric.compute(predictions=val_predicted_labels, references=val_reference_labels)\n",
        "\n",
        "        #         total_val_loss += loss\n",
        "        #         total_val_metric += metrics['matthews_correlation']\n",
        "\n",
        "        #     avg_val_metric = total_val_metric / num_val_batches\n",
        "        #     avg_val_loss = total_val_loss / num_val_batches\n",
        "\n",
        "        #     wandb.log(\n",
        "        #         {\n",
        "        #             \"validation_loss\": avg_val_loss,\n",
        "        #             \"validation_matthews_correlation\": avg_val_metric,\n",
        "        #         },\n",
        "        #         step=global_batch,\n",
        "        #         )\n",
        "            \n",
        "        #     if avg_val_loss < best_val_loss:\n",
        "        #         best_val_batch = global_batch\n",
        "        #         best_val_loss = avg_val_loss\n",
        "        #         model_string = f\"./models/m_{run_name}_{best_val_batch}\"\n",
        "        #         print(f\"Saving model checkpoint to {model_string}\")\n",
        "        #         torch.save(model,              f\"{model_string}_all.cpk\")\n",
        "        #         torch.save(model.state_dict(), f\"{model_string}_state.cpk\")\n",
        "        #         # model.save_pretrained(f\"{model_string}_state.pt\")\n",
        "\n",
        "model_string = f\"./models/m_{run_name}_FINAL\"\n",
        "print(f\"Saving model checkpoint to {model_string}\")\n",
        "torch.save(model,              f\"{model_string}_all.cpk\")\n",
        "torch.save(model.state_dict(), f\"{model_string}_state.cpk\")\n",
        "# model.save_pretrained(f\"{model_string}_state.pt\")\n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "run.finish()  # stop wandb run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffP-VQOyIrJk"
      },
      "source": [
        "To see how your model fared you can compare it to the [GLUE Benchmark leaderboard](https://gluebenchmark.com/leaderboard).\n",
        "\n",
        "### Task:\n",
        "Tune your hparams until you are close to the original BERT results. Feel free to read BERT paper to find what hparam the authors used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WINsjEgNe8Jm"
      },
      "source": [
        "# Interact with your trained model\n",
        "\n",
        "Input some examples into your model and look at the predicitons.\n",
        "Find 4 inputs such that:\n",
        "1. Model makes a correct positive prediction (predicts 1 when the label is 1)\n",
        "1. Model makes an incorrect positive prediction (predicts 1 when the label is 0)\n",
        "1. Model makes a correct negative prediction (predicts 0 when the label is 0)\n",
        "1. Model makes an incorrect negative prediciton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQxCOYvVe7pq"
      },
      "outputs": [],
      "source": [
        "# Task 1.13\n",
        "# Interact with your trained model and find true/false positive/negatives\n",
        "# Freel free to either come up with your own examples or to find them in the test set (but **not** from the train set).\n",
        "# You can use more than one notebook cell for this task\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MAhwGTEe2Vi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFZvZbtRHNFo"
      },
      "source": [
        "# The final task (1.14)\n",
        "\n",
        "\n",
        "Now, instead of using a pre-trained BERT, let's initialize it with random values and train our classifier from scratch. This way we'll see how much improvemet we get from the pre-training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_bLxGlqgrse"
      },
      "source": [
        "Now, please **copy** the cell with the training loop below (yes, this is generaly a bad practice, but it would help us to check your homework easier in this particular case)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDYZ_jnqgZc8"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(project=f\"bert_classification_{task}\")\n",
        "\n",
        "# Feel free to modify the code below\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model_config = AutoConfig.from_pretrained(model_name)  # we request a config of exactly the same model as we worked with before\n",
        "bert_untrained = AutoModel(model_config)  # create the model from config, no pre-trained weights provided\n",
        "model = BertForClassification(pre_trained_encoder=bert_untrained, num_classes=num_labels)\n",
        "model = model.to(device)\n",
        "\n",
        "# Training loop\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "run.finish()  # stop wandb run"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "hw6_bert_classification.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "f86d5ac646110a331c60478f9400a1a64d0cd523be90d887457228f9723636b5"
    },
    "kernelspec": {
      "display_name": "Python 3.7.11 ('nlp_class')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
