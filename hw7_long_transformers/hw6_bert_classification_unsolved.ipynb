{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkV5i3_TYUIw"
      },
      "outputs": [],
      "source": [
        "# Copyright 2020 The HuggingFace Team. All rights reserved.\n",
        "# Copyright 2022 Vladislav Lialin\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4cRE8IbIrIV"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to\n",
        "\n",
        "1. install ðŸ¤— Transformers and ðŸ¤— Datasets. Uncomment the following cell and run it.\n",
        "2. make sure your runtime is GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "outputs": [],
      "source": [
        "! pip install datasets transformers wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfkbFABAYgms"
      },
      "outputs": [],
      "source": [
        "# Uncomment this to verify that you have a GPU\n",
        "# !nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V29soEZFYUJD"
      },
      "source": [
        "Make sure your version of Transformers is at least 4.11.0 since the functionality was introduced in that version:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "nUpYMxzRYUJE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.27.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import transformers\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Fine-tuning a model on a text classification task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTCFado4IrIc"
      },
      "source": [
        "In this notebook, we will see how to fine-tune one of the [ðŸ¤— Transformers](https://github.com/huggingface/transformers) model to a text classification task of the [GLUE Benchmark](https://gluebenchmark.com/).\n",
        "\n",
        "The GLUE Benchmark is a group of nine classification tasks on sentences or pairs of sentences which are:\n",
        "\n",
        "- [CoLA](https://nyu-mll.github.io/CoLA/) (Corpus of Linguistic Acceptability) Determine if a sentence is grammatically correct or not.is a  dataset containing sentences labeled grammatically correct or not.\n",
        "- [MNLI](https://arxiv.org/abs/1704.05426) (Multi-Genre Natural Language Inference) Determine if a sentence entails, contradicts or is unrelated to a given hypothesis. (This dataset has two versions, one with the validation and test set coming from the same distribution, another called mismatched where the validation and test use out-of-domain data.)\n",
        "- [MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398) (Microsoft Research Paraphrase Corpus) Determine if two sentences are paraphrases from one another or not.\n",
        "- [QNLI](https://rajpurkar.github.io/SQuAD-explorer/) (Question-answering Natural Language Inference) Determine if the answer to a question is in the second sentence or not. (This dataset is built from the SQuAD dataset.)\n",
        "- [QQP](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs) (Quora Question Pairs2) Determine if two questions are semantically equivalent or not.\n",
        "- [RTE](https://aclweb.org/aclwiki/Recognizing_Textual_Entailment) (Recognizing Textual Entailment) Determine if a sentence entails a given hypothesis or not.\n",
        "- [SST-2](https://nlp.stanford.edu/sentiment/index.html) (Stanford Sentiment Treebank) Determine if the sentence has a positive or negative sentiment.\n",
        "- [WNLI](https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html) (Winograd Natural Language Inference) Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed or not. (This dataset is built from the Winograd Schema Challenge dataset.)\n",
        "\n",
        "> We do not use STSB here, because it uses slightly different evaluation schema than the rest and we don't want to complicate code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "YZbiBDuGIrId"
      },
      "outputs": [],
      "source": [
        "GLUE_TASKS = [\"cola\", \"mnli\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"wnli\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "zVvslsfMIrIh"
      },
      "outputs": [],
      "source": [
        "# Task 1.1: Select a task (from GLUE_TASKS) and a model\n",
        "# We recommend \"cola\" and \"bert-base-uncased\"\n",
        "# You can also try \"bert-large-uncased\" (a larger model) and \"distilbert-base-cased\" (a smaller model)\n",
        "# Full list of models: https://huggingface.co/transformers/pretrained_models.html (not all of them will work with this script)\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "task = \"cola\"\n",
        "model_name = \"bert-base-uncased\"\n",
        "# YOUR CODE ENDS HERE\n",
        "assert task in GLUE_TASKS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7QYTpxXIrIl"
      },
      "source": [
        "We will use the [ðŸ¤— Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions `load_dataset` and `load_metric`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IreSlFmlIrIm"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKx2zKs5IrIq"
      },
      "source": [
        "We can directly pass our task name to those functions. `load_dataset` will cache the dataset to avoid downloading it again the next time you run this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "s_AY1ATSIrIq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset glue (C:/Users/chris/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "881339ff77704061ae413bb961ce2ac5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = load_dataset(\"glue\", task)\n",
        "metric = load_metric(\"glue\", task)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzfPtOMoIrIu"
      },
      "source": [
        "The `dataset` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set (with more keys for the mismatched validation and test set in the special case of `mnli`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "GWiVUF0jIrIv"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence', 'label', 'idx'],\n",
              "        num_rows: 8551\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['sentence', 'label', 'idx'],\n",
              "        num_rows: 1043\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence', 'label', 'idx'],\n",
              "        num_rows: 1063\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random\n",
        "random.randint(0,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "Pe4NNJepYUJK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'sentence': 'That guy, who I think might be drunk, just hit me!', 'label': 1, 'idx': 6180}\n",
            "{'sentence': \"Where's the bikini which Tom mentioned the fact that Sue had worn?\", 'label': 0, 'idx': 1093}\n",
            "{'sentence': \"Janet broke Bill's finger.\", 'label': 1, 'idx': 2158}\n",
            "{'sentence': 'Cora coiled the rope around the post.', 'label': 1, 'idx': 2583}\n",
            "{'sentence': 'Although no one else believes it, Harry believes that Sally is innocent.', 'label': 1, 'idx': 1585}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[None, None, None, None, None]"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Task 1.1: To get a sense of what the data looks like, print 5 random examples from the \"train\" subset\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "num_samples = len(dataset[\"train\"])\n",
        "[print(dataset[\"train\"][random.randint(0,num_samples)]) for _ in range(0,5)]\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnjDIuQ3IrI-"
      },
      "source": [
        "The metric is an instance of [`datasets.Metric`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Metric):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "5o4rUteaIrI_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Metric(name: \"glue\", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: \"\"\"\n",
              "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
              "Args:\n",
              "    predictions: list of predictions to score.\n",
              "        Each translation should be tokenized into a list of tokens.\n",
              "    references: list of lists of references for each translation.\n",
              "        Each reference should be tokenized into a list of tokens.\n",
              "Returns: depending on the GLUE subset, one or several of:\n",
              "    \"accuracy\": Accuracy\n",
              "    \"f1\": F1 score\n",
              "    \"pearson\": Pearson Correlation\n",
              "    \"spearmanr\": Spearman Correlation\n",
              "    \"matthews_correlation\": Matthew Correlation\n",
              "Examples:\n",
              "\n",
              "    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
              "    >>> references = [0, 1]\n",
              "    >>> predictions = [0, 1]\n",
              "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
              "    >>> print(results)\n",
              "    {'accuracy': 1.0}\n",
              "\n",
              "    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
              "    >>> references = [0, 1]\n",
              "    >>> predictions = [0, 1]\n",
              "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
              "    >>> print(results)\n",
              "    {'accuracy': 1.0, 'f1': 1.0}\n",
              "\n",
              "    >>> glue_metric = datasets.load_metric('glue', 'stsb')\n",
              "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
              "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
              "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
              "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
              "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
              "\n",
              "    >>> glue_metric = datasets.load_metric('glue', 'cola')\n",
              "    >>> references = [0, 1]\n",
              "    >>> predictions = [0, 1]\n",
              "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
              "    >>> print(results)\n",
              "    {'matthews_correlation': 1.0}\n",
              "\"\"\", stored examples: 0)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAWdqcUBIrJC"
      },
      "source": [
        "You can call its `compute` method with your predictions and labels directly and it will return a dictionary with the metric(s) value:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "6XN1Rq0aIrJC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'matthews_correlation': 0.29277002188455997}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Task 1.2: Compute the metric using `fake_preds` and `fake_labels`\n",
        "# YOUR CODE STARTS HERE\n",
        "# feel free to modify fake_preds and fake_labels if your metric does not work for these inputs\n",
        "fake_preds = np.array([1, 2, 0, 0, 0, 1, 1, 2])\n",
        "fake_labels = np.array([1, 1, 0, 2, 0, 2, 2, 2])\n",
        "\n",
        "metric_value = metric.compute(predictions=fake_preds, references=fake_labels)\n",
        "print(metric_value)\n",
        "# YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOCrQwPoIrJG"
      },
      "source": [
        "Note that `load_metric` has loaded the proper metric associated to your task, which is:\n",
        "\n",
        "- for CoLA: [Matthews Correlation Coefficient](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient)\n",
        "- for MNLI (matched or mismatched): Accuracy\n",
        "- for MRPC: Accuracy and [F1 score](https://en.wikipedia.org/wiki/F1_score)\n",
        "- for QNLI: Accuracy\n",
        "- for QQP: Accuracy and [F1 score](https://en.wikipedia.org/wiki/F1_score)\n",
        "- for RTE: Accuracy\n",
        "- for SST-2: Accuracy\n",
        "- for WNLI: Accuracy\n",
        "\n",
        "so the metric object only computes the one(s) needed for your task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVx71GdAIrJH"
      },
      "source": [
        "Before we can feed those texts to our model, we need to preprocess them. This is done by a ðŸ¤— Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
        "\n",
        "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
        "\n",
        "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
        "- we download the vocabulary used when pretraining this specific checkpoint.\n",
        "\n",
        "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Task 1.3: Load a pre-trained tokenizer.\n",
        "# Remember that we want to work with a pre-trained model,\n",
        "# which means that we need to use exactly the same tokenizer that was used to pre-train the model.\n",
        "# Provide your model name to the .from_pretrained method and it will download the right tokenizer files for you.\n",
        "# Provide `use_fast=True` to use the fast tokenizer.\n",
        "# You can learn more about fast tokenizers vs slow tokenizers here: https://www.youtube.com/watch?v=g8quOxoqhHQ\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl6IidfdIrJK"
      },
      "source": [
        "We pass along `use_fast=True` to the call above to use one of the fast tokenizers (backed by Rust) from the ðŸ¤— Tokenizers library. Those fast tokenizers are available for almost all models, but if you got an error with the previous call, remove that argument."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rowT4iCLIrJK"
      },
      "source": [
        "You can directly call this tokenizer on one sentence or a pair of sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "a5hBlsrHIrJL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7592, 1010, 2023, 2028, 6251, 999, 102, 1998, 2023, 6251, 3632, 2007, 2009, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(\"Hello, this one sentence!\", text_pair=\"And this sentence goes with it.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo_0B1M2IrJM"
      },
      "source": [
        "Depending on the model you selected, you will see different keys in the dictionary returned by the cell above. They don't matter much for what we're doing here (just know they are required by the model we will instantiate later), you can learn more about them in [this tutorial](https://huggingface.co/transformers/preprocessing.html) if you're interested.\n",
        "\n",
        "To preprocess our dataset, we will thus need the names of the columns containing the sentence(s). The following dictionary keeps track of the correspondence task to column names:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "fyGdtK9oIrJM"
      },
      "outputs": [],
      "source": [
        "task_to_keys = {\n",
        "    \"cola\": (\"sentence\", None),\n",
        "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
        "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
        "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
        "    \"qnli\": (\"question\", \"sentence\"),\n",
        "    \"qqp\": (\"question1\", \"question2\"),\n",
        "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
        "    \"sst2\": (\"sentence\", None),\n",
        "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
        "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbqtC4MrIrJO"
      },
      "source": [
        "We can double check it does work on our current dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "19GG646uIrJO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence: Our friends won't buy this analysis, let alone the next one we propose.\n"
          ]
        }
      ],
      "source": [
        "sentence1_key, sentence2_key = task_to_keys[task]\n",
        "if sentence2_key is None:\n",
        "    print(f\"Sentence: {dataset['train'][0][sentence1_key]}\")\n",
        "else:\n",
        "    print(f\"Sentence 1: {dataset['train'][0][sentence1_key]}\")\n",
        "    print(f\"Sentence 2: {dataset['train'][0][sentence2_key]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C0hcmp9IrJQ"
      },
      "source": [
        "We can them write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True`. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "vc0BSBLIIrJQ"
      },
      "outputs": [],
      "source": [
        "# Task 1.4: Implement preprocess_function that applies tokenizer to either a single sentence or to a pair of sentences.\n",
        "# Truncate the sequences via passing `truncation=True` to the tokenizer in case some texts have larger length that the model can process (512 tokens).\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "def preprocess_function(sample, task):\n",
        "    sentence1_key, sentence2_key= task_to_keys[task]\n",
        "    if sentence2_key is None:\n",
        "        # Tokenize a single sentence\n",
        "        tokenized = tokenizer(sample[sentence1_key], truncation=True)\n",
        "        return tokenized\n",
        "\n",
        "    # Tokenize a pair of sentences: one defined by sentence1_key, the other defined by sentence2_key\n",
        "    tokenized = [tokenizer(sample[sentence1_key], truncation=True),\n",
        "                 \n",
        "\n",
        "    return tokenized\n",
        "# YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lm8ozrJIrJR"
      },
      "source": [
        "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'sentence': 'The more you eat, the less you want.', 'label': 1, 'idx': 99}"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ZONA - delete me\n",
        "dataset[\"train\"][99]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "-b70jh26IrJS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'sentence': [\"Our friends won't buy this analysis, let alone the next one we propose.\", \"One more pseudo generalization and I'm giving up.\", \"One more pseudo generalization or I'm giving up.\", 'The more we study verbs, the crazier they get.', 'Day by day the facts are getting murkier.'], 'label': [1, 1, 1, 1, 1], 'idx': [0, 1, 2, 3, 4]}\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Task 1.5 to make sure your preprocess_function works, apply it to the first 5 elements of the train set\n",
        "# You can get them vias dataset[\"train\"][:5]\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "y = preprocess_function(dataset[\"train\"][0:5])\n",
        "pprint(y)\n",
        "#y = [preprocess_function(x) for x in dataset[\"train\"][:5]]\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS-6iXTkIrJT"
      },
      "source": [
        "To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command.\n",
        "\n",
        "Because you apply this fuction to the `dataset` object that contains all `train`, `valid` and `test` subsets,\n",
        "encoded_dataset will have all of the same subsets.\n",
        "use `remove_columns` to remove all of the columns that are not needed for the model\n",
        "(all `dataset[\"train\"].column_names`) except `label` (we have already created one for you -- `old_column_names`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDtsaJeVIrJT"
      },
      "outputs": [],
      "source": [
        "old_column_names = [c for c in dataset[\"train\"].column_names if c != \"label\"]\n",
        "\n",
        "# Task 1.6: map the preprocess_function to the dataset\n",
        "# Use batched=True, so it would run faster\n",
        "# (batched operations are always faster than a single-element ones, even though they are a bit harder to write)\n",
        "# Assign the results to encoded_dataset\n",
        "# YOUR CODE STARTS HERE\n",
        "encoded_dataset =\n",
        "# YOUR CODE ENDS HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOYLqG0fYUJR"
      },
      "outputs": [],
      "source": [
        "encoded_dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voWiw8C7IrJV"
      },
      "source": [
        "The `.map` results are automatically cached by the ðŸ¤— Datasets library to avoid spending time on this step the next time you run your notebook. The ðŸ¤— Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). For instance, it will properly detect if you change the task in the first cell and rerun the notebook. ðŸ¤— Datasets warns you when it uses cached files, you can pass `load_from_cache_file=False` in the call to `map` to not use the cached files and force the preprocessing to be applied again.\n",
        "\n",
        "Note that we passed `batched=True` to encode the texts by batches together. This is to leverage the full benefit of the fast tokenizer we loaded earlier, which will use multi-threading to treat the texts in a batch concurrently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBiW8UpKIrJW"
      },
      "source": [
        "Now that our data is ready, we can download the pretrained model and fine-tune it. Since all our tasks are about sentence classification, we use the `AutoModel` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us. This model will return us the hidden states of the pre-trained transformers. We need to take them and use to predict the classes. To do that we need to pool them across the time dimension (to get a fixed-size vector independent on the sequence length). The most common way to do that is to wrap the model into a new `nn.Module` class that will have our pre-trained transformer and an extra linear layer (feel free to use more than one layer).\n",
        "\n",
        "But first, let's load the model using `AutoModel.from_pretrained` and play with it to learn its interfaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlqNaB8jIrJW"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "# Task 1.7: Load a pre-trained model using the same name you used for the tokenizer\n",
        "# name it bert_model\n",
        "# YOUR CODE STARTS HERE\n",
        "bert_model =\n",
        "# YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7BVl5vSYUJS"
      },
      "source": [
        "To tokenize the sentence (or a pair of sentences), we use a tokenizer.\n",
        "More that that, we can ask the tokenizer to return us pytorch tensors if we provide `return_tensors=\"pt\"`, where \"pt\" means PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXSfLOEqYUJS"
      },
      "outputs": [],
      "source": [
        "texts = [\"Hello, this is the sentence number one!\",\n",
        "         \"This is another sentence!\"]\n",
        "text_pairs = [\"And this sentence goes with the first sentence.\",\n",
        "              \"And this sentence goes with the second one.\"]\n",
        "\n",
        "# Task 1.8: Tokenize the batch of texts and text pairs\n",
        "# Note that you need to make the tokenizer to\n",
        "# 1. return pytorch tensors\n",
        "# 2. do padding\n",
        "# If padding is not turned on, the model won't be able to create a batch of these two examples.\n",
        "# YOUR CODE STARTS HERE\n",
        "input_object =\n",
        "# YOUR CODE ENDS HERE\n",
        "input_object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sZf3P2eYUJT"
      },
      "source": [
        "Your `input_object` should look like this:\n",
        "\n",
        "```python\n",
        "{\n",
        "'input_ids': tensor(\n",
        "        [[ 101, 7592, 1010, 2023, 2003, 1996, 6251, 2193, 2028,  999,  102, 1998, 2023, 6251, 3632, 2007, 1996, 2034, 6251, 1012,  102],\n",
        "        [ 101, 2023, 2003, 2178, 6251,  999,  102, 1998, 2023, 6251, 3632, 2007, 1996, 2117, 2028, 1012,  102,    0,    0,    0,    0]]), \n",
        "'token_type_ids': tensor(\n",
        "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]),\n",
        "'attention_mask': tensor(\n",
        "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdbnD1heYUJT"
      },
      "source": [
        "We can see that a tokenizer returns a dictionary with the following keys:\n",
        "    \n",
        "    - \"input_ids\": a list of token ids\n",
        "    - \"attention_mask\": a list of booleans, where 1 means that the corresponding token is part of the input, and 0 means that it is padding\n",
        "    - \"token_type_ids\": a list of integers, where 0 means that the corresponding token is the first sentence and 1 means that the corresponding token is the second sentence. These are very imporant for tasks like MNLI or SST.\n",
        "    \n",
        "The model expects the arguments with the same keys, so we can forward it with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZRbrfZKYUJT"
      },
      "outputs": [],
      "source": [
        "ouput_obj = bert_model(\n",
        "    input_ids=input_object[\"input_ids\"],\n",
        "    attention_mask=input_object[\"attention_mask\"],\n",
        "    token_type_ids=input_object[\"token_type_ids\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ko8KbUugYUJU"
      },
      "outputs": [],
      "source": [
        "# Now, look at ouput_obj and figure out how to get the hidden states of the last transformer layer.\n",
        "# Task 1.9: Get the hidden states of the last transformer layer\n",
        "# YOUR CODE STARTS HERE\n",
        "last_hidden_state =\n",
        "# YOUR CODE ENDS HERE\n",
        "\n",
        "_batch_size = len(input_object[\"input_ids\"])\n",
        "_seq_len = len(input_object[\"input_ids\"][0])\n",
        "_hidden = bert_model.config.hidden_size\n",
        "assert last_hidden_state.shape == (_batch_size, _seq_len, _hidden)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdjq7eFJYUJU"
      },
      "source": [
        "Now it's time to wrap the model into PyTorch `nn.Module`.\n",
        "Your task is to:\n",
        "\n",
        "1. Create BertForClassification class that inherits from `nn.Module`\n",
        "2. Implement `__init__` which should take `pre_trained_encoder` (an object of type `BertModel`) and `num_classes` (integer). Then, as usual, you assign the layer objects (and create new ones, if nesessary) to the object arguments (e.g., `self.pre_trained_encoder` and `self.output_layer`).\n",
        "3. Implement `forward()`. Remember to pool the sequence dimension via extracting the first token hidden (it corresponds to the CLS token).\n",
        "\n",
        "This model should take `input_ids`, `attention_mask`, and `token_type_ids` and to output the logits of the `num_classes`.\n",
        "\n",
        "> DO NOT hard-code any values (e.g., do not assume that model hidden is always equal to 768). Extract this number from the model config as we did in the previuos cell.\n",
        "\n",
        "\n",
        "**Extra points:** Feel free to have more than one layer after the pre_trained_encoder and/or to use Batch Normalization, Dropout, and other tricks. Remember that you need nonlinearities between linear layers, but not after the last layer. You can additionally use different learning rates for the body and the head of the model. You can learn more about common tricks in [fast.ai Lesson 1](https://course.fast.ai/videos/?lesson=1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dDn5347YUJU"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE STARTS HERE\n",
        "# (our implementation is 11 lines)\n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "\n",
        "bert_model = AutoModel.from_pretrained(model_name)\n",
        "model = BertForClassification(pre_trained_encoder=bert_model, num_classes=17)\n",
        "output_logits = model(input_object[\"input_ids\"], input_object[\"attention_mask\"], input_object[\"token_type_ids\"])\n",
        "assert output_logits.shape == (2, 17)\n",
        "print(\"Passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CczA5lJlIrJX"
      },
      "source": [
        "The warning is telling us we are throwing away some weights. This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjeeDN55YUJV"
      },
      "outputs": [],
      "source": [
        "num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2\n",
        "metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\n",
        "\n",
        "# Feel free to change the parameters below\n",
        "# YOUR CODE STARTS HERE\n",
        "learning_rate = 2e-5\n",
        "batch_size = 16\n",
        "num_train_epochs = 5\n",
        "weight_decay = 0.01\n",
        "# YOUR CODE ENDS HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N8urzhyIrJY"
      },
      "source": [
        "Now, it's time to create the dataloaders that will batch the data from our datasets.\n",
        "\n",
        "To do that, you need a collation function that combines multiplle examples (of different lengths in general) into a batch. In machine translation homework we had written our own collation function and it looked like this:\n",
        "\n",
        "```python\n",
        "def collation_function_for_seq2seq(batch, source_pad_token_id, target_pad_token_id):\n",
        "    input_ids_list = [ex[\"input_ids\"] for ex in batch]\n",
        "    decoder_input_ids_list = [ex[\"decoder_input_ids\"] for ex in batch]\n",
        "    labels_list = [ex[\"labels\"] for ex in batch]\n",
        "\n",
        "    collated_batch = {\n",
        "        \"input_ids\": pad(input_ids_list, source_pad_token_id),\n",
        "        \"decoder_input_ids\": pad(decoder_input_ids_list, target_pad_token_id),\n",
        "        \"labels\": pad(labels_list, target_pad_token_id),\n",
        "    }\n",
        "\n",
        "    collated_batch[\"encoder_padding_mask\"] = collated_batch[\"input_ids\"] == source_pad_token_id\n",
        "    return collated_batch\n",
        "\n",
        "def pad(sequence_list, pad_id):\n",
        "    max_len = max(len(x) for x in sequence_list)\n",
        "    padded_sequence_list = []\n",
        "    for sequence in sequence_list:\n",
        "        padding = [pad_id] * (max_len - len(sequence))\n",
        "        padded_sequence = sequence + padding\n",
        "        padded_sequence_list.append(padded_sequence)\n",
        "\n",
        "    return torch.LongTensor(padded_sequence_list)\n",
        "```\n",
        "\n",
        "We can either write a similar function by here or to use `transformers.data.data_collator.DataCollatorWithPadding`.\n",
        "\n",
        "> NOTE: remember to shuffle the order of the dataset elements in your training dataloader (and do not shuffle test dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvnbMXPyYUJW"
      },
      "outputs": [],
      "source": [
        "from transformers.data.data_collator import DataCollatorWithPadding\n",
        "\n",
        "# Task 1.10: Create collator, training set dataloader, and validation set dataloader\n",
        "# (these dataloaders should use the collator to collate the examples of different lengths into batches)\n",
        "# YOUR CODE STARTS HERE (our implementation is 3 lines)\n",
        "collator =\n",
        "train_dataloader =\n",
        "valid_dataloader =\n",
        "# YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1GdLQKhYUJW"
      },
      "source": [
        "> If dataloader fails, it might happen because it is trying to collate non-collatable objects like strings. This could happend if you forgot to remove the extra columns from the dataset during `.map` a few cells above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ToKXfT0YUJW"
      },
      "outputs": [],
      "source": [
        "# make sure this line does not fail and gives you a reasonable output\n",
        "batch = next(iter(train_dataloader))\n",
        "batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8ez_CSkYUJW"
      },
      "source": [
        "Now it's time to build a training loop. It will be similar (but not completely similar) to the one you built in the previous homework.\n",
        "\n",
        "1. iterate over the number of epochs,\n",
        "1. iterate over your training examples in the train_dataloader,\n",
        "1. compute the loss and update model parameters,\n",
        "1. in the end of epoch, compute the metric (using `metric`` object) on the validation set.\n",
        "1. **after all training is done, print the final validation metric**\n",
        "\n",
        "Log your training loss and accuracy and your validtion accuracy to wandb (you will need to submit a link to the most succesfull wandb run).\n",
        "\n",
        "You can use `tqdm` to make a progress bar for your epochs and/or iterations within one epohc (over `train_dataloader`).\n",
        "\n",
        "It is not nesessary, but recommended to wrap evaluation loop into a block\n",
        "\n",
        "```python\n",
        "    with torch.no_grad():\n",
        "        ...\n",
        "```\n",
        "\n",
        "This can make evaluation a bit faster, because pytorch won't track the gradient values. \n",
        "\n",
        "> Remember to put your model into eval mode in the beginning of the evaluation and to put it back into train mode at the end or the model would use dropout during evaluatoin which will **very significantly** affect your results.\n",
        "\n",
        "> If your epoch for CoLA takes **more than 5 minutes**, you are probably not utilizing the GPU (forgot to move your model and data to the correct device).\n",
        "\n",
        "Please refer to [this guide](https://docs.wandb.ai/guides/track/jupyter) on how to work with wandb inside jupyter notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcIm3YRrTWwe"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "# random classifier metric on the eval dataset\n",
        "_labels = encoded_dataset[\"validation\"][\"label\"]\n",
        "_shuffled_labels = _labels.copy()\n",
        "random.shuffle(_shuffled_labels)\n",
        "\n",
        "_random_metric_value = metric.compute(predictions=_shuffled_labels, references=_labels)\n",
        "print(f\"Random classifier {metric_name} on {task} is {_random_metric_value}. Your model should perform **way** better than that.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoPVHuMkYUJX"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bliy8zgjIrJY"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(project=f\"bert_classification_{task}\")\n",
        "\n",
        "# Feel free to modify the code below\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "bert_model = AutoModel.from_pretrained(model_name)\n",
        "model = BertForClassification(pre_trained_encoder=bert_model, num_classes=num_labels)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Task 1.11: Training loop\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "run.finish()  # stop wandb run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffP-VQOyIrJk"
      },
      "source": [
        "To see how your model fared you can compare it to the [GLUE Benchmark leaderboard](https://gluebenchmark.com/leaderboard).\n",
        "\n",
        "### Task:\n",
        "Tune your hparams until you are close to the original BERT results. Feel free to read BERT paper to find what hparam the authors used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WINsjEgNe8Jm"
      },
      "source": [
        "# Interact with your trained model\n",
        "\n",
        "Input some examples into your model and look at the predicitons.\n",
        "Find 4 inputs such that:\n",
        "1. Model makes a correct positive prediction (predicts 1 when the label is 1)\n",
        "1. Model makes an incorrect positive prediction (predicts 1 when the label is 0)\n",
        "1. Model makes a correct negative prediction (predicts 0 when the label is 0)\n",
        "1. Model makes an incorrect negative prediciton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQxCOYvVe7pq"
      },
      "outputs": [],
      "source": [
        "# Task 1.13\n",
        "# Interact with your trained model and find true/false positive/negatives\n",
        "# Freel free to either come up with your own examples or to find them in the test set (but **not** from the train set).\n",
        "# You can use more than one notebook cell for this task\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MAhwGTEe2Vi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFZvZbtRHNFo"
      },
      "source": [
        "# The final task (1.14)\n",
        "\n",
        "\n",
        "Now, instead of using a pre-trained BERT, let's initialize it with random values and train our classifier from scratch. This way we'll see how much improvemet we get from the pre-training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_bLxGlqgrse"
      },
      "source": [
        "Now, please **copy** the cell with the training loop below (yes, this is generaly a bad practice, but it would help us to check your homework easier in this particular case)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDYZ_jnqgZc8"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(project=f\"bert_classification_{task}\")\n",
        "\n",
        "# Feel free to modify the code below\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model_config = AutoConfig.from_pretrained(model_name)  # we request a config of exactly the same model as we worked with before\n",
        "bert_untrained = AutoModel(model_config)  # create the model from config, no pre-trained weights provided\n",
        "model = BertForClassification(pre_trained_encoder=bert_untrained, num_classes=num_labels)\n",
        "model = model.to(device)\n",
        "\n",
        "# Training loop\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "run.finish()  # stop wandb run"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "hw6_bert_classification.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "f86d5ac646110a331c60478f9400a1a64d0cd523be90d887457228f9723636b5"
    },
    "kernelspec": {
      "display_name": "Python 3.7.11 ('nlp_class')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
